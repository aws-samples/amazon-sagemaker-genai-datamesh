{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0855b561",
   "metadata": {},
   "source": [
    "# Integrate Modern Data Architectures with Generative AI and interact using prompts for querying SQL databases & APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26105729-b3e3-42d0-a583-8446fff89277",
   "metadata": {},
   "source": [
    "This notebook demonstrates how large language models, such as Anthropic, interact with AWS databases, data stores, and third-party data warehousing solutions like Snowflake. We showcase this interaction by generating and running SQL queries and making requests to API endpoints. We achieve all of this by using the LangChain framework, which allows the language model to interact with its environment and connect with other sources of data. The LangChain framework operates based on the following principles: calling out to a language model, being data-aware, and being agentic. Our notebook focuses on establishing database connections to various data sources, consolidating metadata, and returning fact-based data points in response to user queries using LLMs and LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310d6ea-2ee1-4979-bb5e-b65cb892c0cd",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/GenAI+SQL-Prompt Engineering Steps V6.drawio.png' width=\"800\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0297e0-f2dd-464b-9254-6693c45ebafc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Step 1. Connection to various channels through which LLMs can talk to your data. These channels include:\n",
    "\n",
    "    - RedShift Serverless - to connect to datastore 'tickit'(ticket is referred as tickit in the sample data store) to retrieve information regarding ticket sales.\n",
    "    - Aurora - MySQL Serverless - to connect to datastore that hosts information about the employees.\n",
    "    - S3/Athena - to connect to the SageMaker's offline feature store on claims information. \n",
    "    - Snowflake - to connect to stocks related data residing in finance schema of 3rd party software.\n",
    "    - APIs - to connect to meteo(in this example we use Langchain's sample dataset on meteo) to retrieve weather information.\n",
    "    \n",
    "Step 2. Usage of Dynamic generation of prompt templates by populating metadata of the tables using Glue Data Catalog(GDC) as context. GDC was populated by running a crawler on the databases. Refer to the information here to create and run a glue crawler. In case of api, a line item was created in GDC data extract.\n",
    "\n",
    "Step 3. Apply user query to LLM and Langchain to determine the data channel.\n",
    "\n",
    "Step 4. After determining the data channel, run the Langchain SQL Database chain to convert 'text to sql' and run the query against the source data channel. \n",
    "\n",
    "Finally, display the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0986ea2-f794-431f-a341-b94f0118cb7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-requisites:\n",
    "1. Use kernel Base Python 3.0\n",
    "2. Setup [Aurora MySQL Serverless database](https://aws.amazon.com/getting-started/hands-on/building-serverless-applications-with-amazon-aurora-serverless/?ref=gsrchandson). Load sample dataset for Human Resource department. Use this notebook to load the data into Aurora MySQL.\n",
    "3. Setup [Redshift Serverless](https://catalog.workshops.aws/redshift-immersion/en-US/lab1). Load sample data for Sales & Marketing. For example, 'sample data dev' for 'tickit' dataset available in RedShift examples.\n",
    "4. Setup External database. In this case, we are using Snowflake account and populating stocks data. Use this notebook to load the data into Snowflake.\n",
    "5. Run the [Glue Crawler](https://catalog.us-east-1.prod.workshops.aws/workshops/71b5bdcf-7eb1-4549-b851-66adc860cd04/en-US/2-studio/1-crawler) on all the databases mentioned above. \n",
    "6. Provide API Keys for LLM. In this case, we are using [Anthropic Claude](https://www.anthropic.com/index/introducing-claude).\n",
    "7. Pip install the required libraries mentioned in requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556eddc-8e45-4e42-9157-213316ec468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "sqlalchemy==1.4.47\n",
    "snowflake-sqlalchemy\n",
    "langchain==0.0.166\n",
    "sqlalchemy-aurora-data-api\n",
    "PyAthena[SQLAlchemy]==2.25.2\n",
    "anthropic\n",
    "redshift-connector==2.0.910\n",
    "sqlalchemy-redshift==0.8.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55d516c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy==1.4.47 in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.4.47)\n",
      "Requirement already satisfied: snowflake-sqlalchemy in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.4.7)\n",
      "Requirement already satisfied: langchain==0.0.166 in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.0.166)\n",
      "Requirement already satisfied: sqlalchemy-aurora-data-api in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: PyAthena[SQLAlchemy]==2.25.2 in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2.25.2)\n",
      "Requirement already satisfied: anthropic in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: redshift-connector==2.0.910 in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.0.910)\n",
      "Requirement already satisfied: sqlalchemy-redshift==0.8.14 in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.8.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from sqlalchemy==1.4.47->-r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (1.2.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (1.10.7)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (3.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (2.28.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (2.8.4)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.166->-r requirements.txt (line 3)) (8.2.2)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /usr/local/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (1.29.110)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (2023.5.0)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /usr/local/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (1.26.110)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from redshift-connector==2.0.910->-r requirements.txt (line 7)) (21.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from redshift-connector==2.0.910->-r requirements.txt (line 7)) (67.6.1)\n",
      "Requirement already satisfied: lxml>=4.6.5 in /usr/local/lib/python3.10/site-packages (from redshift-connector==2.0.910->-r requirements.txt (line 7)) (4.9.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from redshift-connector==2.0.910->-r requirements.txt (line 7)) (2023.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/site-packages (from redshift-connector==2.0.910->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: scramp<1.5.0,>=1.2.0 in /usr/local/lib/python3.10/site-packages (from redshift-connector==2.0.910->-r requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: snowflake-connector-python<4.0.0 in /usr/local/lib/python3.10/site-packages (from snowflake-sqlalchemy->-r requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: aurora-data-api>=0.4.0 in /usr/local/lib/python3.10/site-packages (from sqlalchemy-aurora-data-api->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 6)) (0.24.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 3)) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 3)) (1.9.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector==2.0.910->-r requirements.txt (line 7)) (2.4.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/site-packages (from boto3>=1.26.4->PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/site-packages (from boto3>=1.26.4->PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/site-packages (from botocore>=1.29.4->PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/site-packages (from botocore>=1.29.4->PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 3)) (0.8.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 3)) (3.19.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/site-packages (from pydantic<2,>=1->langchain==0.0.166->-r requirements.txt (line 3)) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.166->-r requirements.txt (line 3)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.166->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: asn1crypto>=1.5.1 in /usr/local/lib/python3.10/site-packages (from scramp<1.5.0,>=1.2.0->redshift-connector==2.0.910->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (1.15.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in /usr/local/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (3.17)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in /usr/local/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: cryptography<41.0.0,>=3.1.0 in /usr/local/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (40.0.1)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in /usr/local/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (23.1.1)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (3.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from httpx->anthropic->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/site-packages (from httpx->anthropic->-r requirements.txt (line 6)) (0.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from packaging->redshift-connector==2.0.910->-r requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0->snowflake-sqlalchemy->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->anthropic->-r requirements.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->anthropic->-r requirements.txt (line 6)) (3.6.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.4->PyAthena[SQLAlchemy]==2.25.2->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 3)) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c153cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import PromptTemplate,SagemakerEndpoint,SQLDatabase, SQLDatabaseChain, LLMChain\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import SQLDatabaseSequentialChain\n",
    "\n",
    "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
    "from langchain.chains import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "# from langchain.llms import Anthropic\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a4144-4053-46c1-ba39-8f64f8fb9e00",
   "metadata": {},
   "source": [
    "The data for this COVID-19 dataset is stored in a public accessible S3 bucket. You can use the following command to explore the dataset in Cloud9 terminal.\n",
    "\n",
    "!aws s3 ls s3://covid19-lake/ --no-sign-request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceedd61d-9c21-45b4-b35e-69e5cd047f11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://covid19-lake/rearc-covid-19-testing-data/json/states_daily/part-00000-3529bb0f-a7fa-4035-974e-656388696749-c000.json to s3://blog-genai-datamesh/covid-dataset/part-00000-3529bb0f-a7fa-4035-974e-656388696749-c000.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://covid19-lake/rearc-covid-19-testing-data/json/states_daily/ s3://blog-genai-mda/covid-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f05601-529a-42d4-8cab-ba0b9445e695",
   "metadata": {},
   "source": [
    "### Read parameters from Cloud Formation stack\n",
    "Some of the resources needed for this notebook such as the LLM model endpoint, the Amazon Glue crawler are created through a cloud formation template. The next block of code extracts the outputs \n",
    "and parameters of the cloud formation stack created from that template to get the value of these parameters.\n",
    "\n",
    "The stack name here should match the stack name you used when creating the cloud formation stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406e3ede-99aa-46e0-b3aa-eb8b65f6cff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if used a different name while creating the cloud formation stack then change this to match the name you used\n",
    "CFN_STACK_NAME = \"genai-mda-cfn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a1efa5-a1ae-4384-b149-4103b70fab56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#boto3.client('cloudformation').describe_stacks(StackName=\"ssome\")\n",
    "stacks = boto3.client('cloudformation').list_stacks()\n",
    "stack_found = CFN_STACK_NAME in [stack['StackName'] for stack in stacks['StackSummaries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0bda7a4-f2a9-4379-b4a9-8e3e17240d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfn outputs={'LLMEndpointName': 'aws-genai-mda-blog-flan-t5-xxl-endpoint-82d8d900', 'SageMakerNotebookURL': 'https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances/openNotebook/aws-genai-mda-blog?view=classic', 'Region': 'us-east-1'}\n",
      "params={'CFNCrawlerName2': 'cfn-crawler-json', 'SageMakerIAMRole': 'awsGenAIMDAblogIAMRole', 'DataBucketName': 'blog-genai-mda', 'SageMakerNotebookName': 'aws-genai-mda-blog', 'CFNTablePrefixName': 'cfn_', 'CFNDatabaseName': 'cfn_covid_lake'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def get_cfn_outputs(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "def get_cfn_parameters(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    params = {}\n",
    "    for param in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Parameters']:\n",
    "        params[param['ParameterKey']] = param['ParameterValue']\n",
    "    return params\n",
    "\n",
    "if stack_found is True:\n",
    "    outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "    params = get_cfn_parameters(CFN_STACK_NAME)\n",
    "    glue_crawler_name = params['CFNCrawlerName2']\n",
    "    glue_database_name = params['CFNDatabaseName']\n",
    "    glue_databucket_name = params['DataBucketName']\n",
    "    Region = outputs['Region']\n",
    "    print(f\"cfn outputs={outputs}\\nparams={params}\")\n",
    "\n",
    "    # ARN of the secret is of the following format arn:aws:secretsmanager:region:account_id:secret:my_path/my_secret_name-autoid\n",
    "    # os_creds_secretid_in_secrets_manager = \"-\".join(outputs['OpenSearchSecret'].split(\":\")[-1].split('-')[:-1])\n",
    "else:\n",
    "    # logger.info(f\"cloud formation stack {CFN_STACK_NAME} not found, set parameters manually here\")\n",
    "    # REPLACE THE \"placeholder\" WITH ACTUAL VALUES IF YOU CREATED THESE RESOURCES WITHOUT USING A CLOUD FORMATION TEMPLATE\n",
    "    embeddings_model_endpoint_name = \"placeholder\"\n",
    "    opensearch_domain_endpoint = \"placeholder\"\n",
    "    opensearch_index = \"placeholder\"\n",
    "    os_creds_secretid_in_secrets_manager = \"placeholder\"\n",
    "    app_name = \"llm-apps-blogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ccbd062-0918-4dcf-a4c2-45bfa81e56c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python_glueworkshop.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile python_glueworkshop.py\n",
    "import boto3\n",
    "import argparse\n",
    "\n",
    "argParser = argparse.ArgumentParser()\n",
    "argParser.add_argument(\"-c\", \"--glue_crawler_name\", help=\"script help\")\n",
    "args = argParser.parse_args()\n",
    "\n",
    "client = boto3.client('glue')\n",
    "\n",
    "# This is the command to start the Crawler\n",
    "try:\n",
    "    response = client.start_crawler(\n",
    "        Name=args.glue_crawler_name \n",
    "    )\n",
    "    print(\"Successfully started crawler. The crawler may take 2-5 mins to detect the schema.\")\n",
    "except:\n",
    "    print(\"error in starting crawler. Check the logs for the error details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1392725-610f-43c1-8fd9-b5f52c4585b1",
   "metadata": {},
   "source": [
    "Execute the python script by passing the glue crawler name from the cloudformation stack output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75d0fa47-1651-4cff-802a-22ae6438a09c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully started crawler\n"
     ]
    }
   ],
   "source": [
    "!python python_glueworkshop.py -c glue_crawler_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d1d0e-33fb-46ca-b82f-6294ea867cae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1 - Connect to databases using SQL Alchemy. \n",
    "\n",
    "Under the hood, LangChain uses SQLAlchemy to connect to SQL databases. The SQLDatabaseChain can therefore be used with any SQL dialect supported by SQLAlchemy, \n",
    "such as MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. Please refer to the SQLAlchemy documentation for more information about requirements for connecting to your database. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5ce28-9b33-4061-8655-2b297d5c24a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Important**: The code below establishes a database connection for data sources and Large Language Models. Please note that the solution will only work if the database connection for your sources is defined in the cell below. Please refer to the Pre-requisites section. If your use case requires data from Aurora MySQL alone, then please comment out other data sources. Furthermore, please update the cluster details and variables for Aurora MySQL accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1583cade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define connections\n",
    "\n",
    "# collect credentials from Secrets Manager\n",
    "client = boto3.client('secretsmanager')\n",
    "\n",
    "#LLM \n",
    "#get the llm api key\n",
    "#llm variables\n",
    "anthropic_secret_id = \"anthropic\"#<your anthropic secret id>\n",
    "## llm get credentials from secrets manager\n",
    "response = client.get_secret_value(SecretId=anthropic_secret_id)\n",
    "secrets_credentials = json.loads(response['SecretString'])\n",
    "ANTHROPIC_API_KEY = secrets_credentials['ANTHROPIC_API_KEY']\n",
    "#define large language model here. Make sure to set api keys for the variable ANTHROPIC_API_KEY\n",
    "llm = ChatAnthropic(temperature=0, anthropic_api_key=ANTHROPIC_API_KEY, max_tokens_to_sample = 512)\n",
    "\n",
    "#S3\n",
    "# connect to s3 using athena\n",
    "## athena variables\n",
    "connathena=f\"athena.{Region}.amazonaws.com\" \n",
    "portathena='443' #Update, if port is different\n",
    "schemaathena=glue_database_name #from cfn params\n",
    "s3stagingathena=f's3://{glue_databucket_name}/athenaresults/'#from cfn params\n",
    "wkgrpathena='primary'#Update, if workgroup is different\n",
    "# tablesathena=['dataset']#[<tabe name>]\n",
    "##  Create the athena connection string\n",
    "connection_string = f\"awsathena+rest://@{connathena}:{portathena}/{schemaathena}?s3_staging_dir={s3stagingathena}/&work_group={wkgrpathena}\"\n",
    "##  Create the athena  SQLAlchemy engine\n",
    "engine_athena = create_engine(connection_string, echo=False)\n",
    "dbathena = SQLDatabase(engine_athena)\n",
    "# dbathena = SQLDatabase(engine_athena, include_tables=tablesathena)\n",
    "# #SNOWFLAKE\n",
    "# # connect to snowflake database\n",
    "# ## snowflake variables\n",
    "# sf_account_id = <your snowflake account id>\n",
    "# sf_secret_id =<your snowflake credentials secret id>\n",
    "# dwh = <your dwh>\n",
    "# db = <your database>\n",
    "# schema = <your database schema>\n",
    "# table = <table name>\n",
    "# ## snowflake get credentials from secrets manager\n",
    "# response = client.get_secret_value(SecretId=sf_secret_id)\n",
    "# secrets_credentials = json.loads(response['SecretString'])\n",
    "# sf_password = secrets_credentials['password']\n",
    "# sf_username = secrets_credentials['username']\n",
    "# ##  Create the snowflake connection string\n",
    "# connection_string = f\"snowflake://{sf_username}:{sf_password}@{sf_account_id}/{db}/{schema}?warehouse={dwh}\"\n",
    "# ##  Create the snowflake  SQLAlchemy engine\n",
    "# engine_snowflake = create_engine(connection_string, echo=False)\n",
    "# dbsnowflake = SQLDatabase(engine_snowflake)\n",
    "\n",
    "# #AURORA MYSQL\n",
    "# ##connect to aurora mysql\n",
    "# ##aurora mysql cluster details/variables\n",
    "# cluster_arn = <your cluster arn>\n",
    "# secret_arn =<your cluster secret arn>\n",
    "# rdsdb=<your database>\n",
    "# rdsdb_tbl = [<table name>]\n",
    "# ##  Create the aurora connection string\n",
    "# connection_string = f\"mysql+auroradataapi://:@/{rdsdb}\"\n",
    "# ##  Create the aurora  SQLAlchemy engine\n",
    "# engine_rds = create_engine(connection_string, echo=False,connect_args=dict(aurora_cluster_arn=cluster_arn, secret_arn=secret_arn))\n",
    "# dbrds = SQLDatabase(engine_rds, include_tables=rdsdb_tbl)\n",
    "\n",
    "# #REDSHIFT\n",
    "# # connect to redshift database\n",
    "# ## redshift variables\n",
    "# rs_secret_id = <redshift secret id>\n",
    "# rs_endpoint=<redshift endpoint>\n",
    "# rs_port=<redshift port>\n",
    "# rs_db=<redshift database>\n",
    "# rs_schema=<redshift database schema>\n",
    "# ## redshift get credentials from secrets manager\n",
    "# response = client.get_secret_value(SecretId=rs_secret_id)\n",
    "# secrets_credentials = json.loads(response['SecretString'])\n",
    "# rs_password = secrets_credentials['password']\n",
    "# rs_username = secrets_credentials['username']\n",
    "# ##  Create the redshift connection string\n",
    "# connection_string = f\"redshift+redshift_connector://{rs_username}:{rs_password}@{rs_endpoint}:{rs_port}/{rs_db}\"\n",
    "# engine_redshift = create_engine(connection_string, echo=False)\n",
    "# dbredshift = SQLDatabase(engine_redshift)\n",
    "\n",
    "#Glue Data Catalog\n",
    "##Provide list of all the databases where the table metadata resides after the glue successfully crawls the table\n",
    "# gdc = ['redshift-sagemaker-sample-data-dev', 'snowflake','rds-aurora-mysql-employees','sagemaker_featurestore'] # mentioned a few examples here\n",
    "gdc = [schemaathena] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea21757-b08a-438b-a5a7-79d85a9a9085",
   "metadata": {},
   "source": [
    "### Step 2 - Generate Dynamic Prompt Templates\n",
    "Build a consolidated view of Glue Data Catalog by combining metadata stored for all the databases in pipe delimited format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08a3373d-9285-4fab-81b5-51e5364590b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3|cfn_covid_lake|cfn_covid_dataset|totaltestresults\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|fips\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|deathincrease\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|hospitalizedincrease\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|negativeincrease\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|positiveincrease\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|totaltestresultsincrease\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|negative\n",
      "s3|cfn_covid_lake|cfn_covid_dataset|pending\n",
      "api|meteo|weather|weather\n"
     ]
    }
   ],
   "source": [
    "#Generate Dynamic prompts to populate the Glue Data Catalog\n",
    "#harvest aws crawler metadata\n",
    "\n",
    "def parse_catalog():\n",
    "    #Connect to Glue catalog\n",
    "    #get metadata of redshift serverless tables\n",
    "    columns_str=''\n",
    "    \n",
    "    #define glue cient\n",
    "    glue_client = boto3.client('glue')\n",
    "    \n",
    "    for db in gdc:\n",
    "        response = glue_client.get_tables(DatabaseName =db)\n",
    "        for tables in response['TableList']:\n",
    "            #classification in the response for s3 and other databases is different. Set classification based on the response location\n",
    "            if tables['StorageDescriptor']['Location'].startswith('s3'):  classification='s3' \n",
    "            else:  classification = tables['Parameters']['classification']\n",
    "            for columns in tables['StorageDescriptor']['Columns']:\n",
    "                    dbname,tblname,colname=tables['DatabaseName'],tables['Name'],columns['Name']\n",
    "                    columns_str=columns_str+f'\\n{classification}|{dbname}|{tblname}|{colname}'                     \n",
    "    #API\n",
    "    ## Append the metadata of the API to the unified glue data catalog\n",
    "    columns_str=columns_str+'\\n'+('api|meteo|weather|weather')\n",
    "    return columns_str\n",
    "\n",
    "glue_catalog = parse_catalog()\n",
    "\n",
    "#display a few lines from the catalog\n",
    "print('\\n'.join(glue_catalog.splitlines()[-10:]) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e6770-42c3-402b-a60e-9c21fb99d5f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3 - Define Functions to 1/ determine the best data channel to answer the user query, 2/ Generate response to  user query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda3714-3f32-4480-9526-91cca37489d1",
   "metadata": {},
   "source": [
    "In this code sample, we use the Anthropic Model to generate inferences. You can utilize SageMaker JumpStart models  to achieve the same. \n",
    "Guidance on how to use the JumpStart Models is available in the notebook - mda_with_llm_langchain_smjumpstart_flant5xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4efcc59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function 1 'Infer Channel'\n",
    "#define a function that infers the channel/database/table and sets the database for querying\n",
    "def identify_channel(query):\n",
    "    #Prompt 1 'Infer Channel'\n",
    "    ##set prompt template. It instructs the llm on how to evaluate and respond to the llm. It is referred to as dynamic since glue data catalog is first getting generated and appended to the prompt.\n",
    "    prompt_template = \"\"\"\n",
    "     From the table below, find the database (in column database) which will contain the data (in corresponding column_names) to answer the question \n",
    "     {query} \\n\n",
    "     \"\"\"+glue_catalog +\"\"\" \n",
    "     Give your answer as database == \n",
    "     Also,give your answer as database.table == \n",
    "     \"\"\"\n",
    "    ##define prompt 1\n",
    "    PROMPT_channel = PromptTemplate( template=prompt_template, input_variables=[\"query\"]  )\n",
    "\n",
    "    # define llm chain\n",
    "    llm_chain = LLMChain(prompt=PROMPT_channel, llm=llm)\n",
    "    #run the query and save to generated texts\n",
    "    generated_texts = llm_chain.run(query)\n",
    "    print(generated_texts)\n",
    "\n",
    "    #set the best channel from where the query can be answered\n",
    "    if 'snowflake' in generated_texts: \n",
    "            channel='db'\n",
    "            db=dbsnowflake \n",
    "            print(\"SET database to snowflake\")  \n",
    "    elif 'redshift'  in generated_texts: \n",
    "            channel='db'\n",
    "            db=dbredshift\n",
    "            print(\"SET database to redshift\")\n",
    "    elif 's3' in generated_texts: \n",
    "            channel='db'\n",
    "            db=dbathena\n",
    "            print(\"SET database to athena\")\n",
    "    elif 'rdsmysql' in generated_texts: \n",
    "            channel='db'\n",
    "            db=dbrds\n",
    "            print(\"SET database to rds\")    \n",
    "    elif 'api' in generated_texts: \n",
    "            channel='api'\n",
    "            print(\"SET database to weather api\")        \n",
    "    else: raise Exception(\"User question cannot be answered by any of the channels mentioned in the catalog\")\n",
    "    print(\"Step complete. Channel is: \", channel)\n",
    "    \n",
    "    return channel, db\n",
    "\n",
    "#Function 2 'Run Query'\n",
    "#define a function that infers the channel/database/table and sets the database for querying\n",
    "def run_query(query):\n",
    "\n",
    "    channel, db = identify_channel(query) #call the identify channel function first\n",
    "\n",
    "    ##Prompt 2 'Run Query'\n",
    "    #after determining the data channel, run the Langchain SQL Database chain to convert 'text to sql' and run the query against the source data channel. \n",
    "    #provide rules for running the SQL queries in default template--> table info.\n",
    "\n",
    "    _DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
    "\n",
    "    Do not append 'Query:' to SQLQuery.\n",
    "    \n",
    "    Display SQLResult after the query is run in plain english that users can understand. \n",
    "\n",
    "    Provide answer in simple english statement.\n",
    " \n",
    "    Only use the following tables:\n",
    "\n",
    "    {table_info}\n",
    "    If someone asks for the sales, they really mean the tickit.sales table.\n",
    "    If someone asks for the sales date, they really mean the column tickit.sales.saletime.\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    "\n",
    "    PROMPT_sql = PromptTemplate(\n",
    "        input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\n",
    "    )\n",
    "\n",
    "    \n",
    "    if channel=='db':\n",
    "        db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT_sql, verbose=True, return_intermediate_steps=False)\n",
    "        response=db_chain.run(query)\n",
    "    elif channel=='api':\n",
    "        chain_api = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n",
    "        response=chain_api.run(query)\n",
    "    else: raise Exception(\"Unlisted channel. Check your unified catalog\")\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a92cd-e1b4-4feb-ab7a-f97030ba7f84",
   "metadata": {},
   "source": [
    "### Step 4 - Run the run_query function that in turn calls the Langchain SQL Database chain to convert 'text to sql' and runs the query against the source data channel\n",
    "\n",
    "Some samples are provided below for test runs. Uncomment the query to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f82599a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " database == s3\n",
      "database.table == cfn_covid_lake.cfn_covid_dataset\n",
      "SET database to athena\n",
      "Step complete. Channel is:  db\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Which States reported the least and maximum deaths?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m SELECT state, MIN(death) AS least_deaths, MAX(death) AS max_deaths \n",
      "FROM cfn_covid_dataset \n",
      "GROUP BY state \n",
      "ORDER BY least_deaths, max_deaths\n",
      "\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('AS', 0.0, 0.0), ('MP', 0.0, 2.0), ('AK', 0.0, 305.0), ('WY', 0.0, 682.0), ('DC', 0.0, 1030.0), ('DE', 0.0, 1473.0), ('ND', 0.0, 1478.0), ('ID', 0.0, 1879.0), ('UT', 0.0, 1976.0), ('NE', 0.0, 2113.0), ('WV', 0.0, 2325.0), ('NV', 0.0, 5037.0), ('AR', 0.0, 5417.0), ('MO', 0.0, 8161.0), ('AL', 0.0, 10149.0), ('NC', 0.0, 11502.0), ('IN', 0.0, 12737.0), ('AZ', 0.0, 16328.0), ('MI', 0.0, 16658.0), ('NJ', 0.0, 23574.0), ('VI', 1.0, 25.0), ('GU', 1.0, 133.0), ('HI', 1.0, 445.0), ('ME', 1.0, 706.0), ('NH', 1.0, 1184.0), ('MT', 1.0, 1381.0), ('SD', 1.0, 1900.0), ('PR', 1.0, 2059.0), ('NM', 1.0, 3808.0), ('OK', 1.0, 4534.0), ('KS', 1.0, 4816.0), ('KY', 1.0, 4819.0), ('IA', 1.0, 5558.0), ('CO', 1.0, 5989.0), ('MN', 1.0, 6550.0), ('MS', 1.0, 6808.0), ('CT', 1.0, 7704.0), ('SC', 1.0, 8754.0), ('VA', 1.0, 9596.0), ('OH', 1.0, 17656.0), ('GA', 1.0, 17906.0), ('IL', 1.0, 23014.0), ('PA', 1.0, 24349.0), ('TX', 1.0, 44451.0), ('VT', 2.0, 208.0), ('WA', 2.0, 5041.0), ('MD', 2.0, 7955.0), ('LA', 2.0, 9748.0), ('TN', 2.0, 11543.0), ('MA', 2.0, 16417.0), ('FL', 2.0, 32266.0), ('OR', 3.0, 2296.0), ('RI', 3.0, 2541.0), ('WI', 3.0, 7106.0), ('NY', 3.0, 39029.0), ('CA', 4.0, 54124.0)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m The states that reported the least deaths are American Samoa, Northern Mariana Islands and Alaska with 0 deaths.\n",
      "The states that reported the maximum deaths are California with 54,124 deaths, New York with 39,029 deaths and Florida with 32,266 deaths.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "SQL and response from user query Which States reported the least and maximum deaths?  \n",
      "   The states that reported the least deaths are American Samoa, Northern Mariana Islands and Alaska with 0 deaths.\n",
      "The states that reported the maximum deaths are California with 54,124 deaths, New York with 39,029 deaths and Florida with 32,266 deaths.\n"
     ]
    }
   ],
   "source": [
    "# Enter the query\n",
    "## Few queries to try out - \n",
    "#athena - Healthcare - Covid dataset\n",
    "# query = \"\"\"How many covid hospitalizations were reported in NY in June of 2021?\"\"\"  \n",
    "query = \"\"\"Which States reported the least and maximum deaths?\"\"\" \n",
    "\n",
    "#snowflake - Finance and Investments\n",
    "# query = \"\"\"Which stock performed the best and the worst in May of 2013?\"\"\"\n",
    "# query = \"\"\"What is the average volume stocks traded  in July of 2013?\"\"\"\n",
    "\n",
    "#rds - Human Resources\n",
    "# query = \"\"\"Name all employees with birth date this month\"\"\" \n",
    "# query = \"\"\"Combien d'employés sont des femmes? \"\"\" #Ask question in French - How  many females are there?\n",
    "# query = \"\"\"How many employees were hired before 1990?\"\"\"  \n",
    "\n",
    "#athena - Legal - SageMaker offline featurestore\n",
    "# query = \"\"\"How many frauds happened in the year 2023 ?\"\"\"  \n",
    "# query = \"\"\"How many policies were claimed this year ?\"\"\" \n",
    "\n",
    "#redshift - Sales & Marketing\n",
    "# query = \"\"\"How many tickit sales are there\"\"\" \n",
    "# query = \"what was the total commision for the tickit sales in the year 2008?\" \n",
    "\n",
    "#api - product - weather\n",
    "# query = \"\"\"What is the weather like right now in New York City in degrees Farenheit?\"\"\"\n",
    "\n",
    "#Response from Langchain\n",
    "response =  run_query(query)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(f'SQL and response from user query {query}  \\n  {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69371bdc-537f-4e5e-a004-99d852097862",
   "metadata": {},
   "source": [
    "### Clean-up\n",
    "After you run the modern data architecture with Generative AI, make sure to clean up any resources that won’t be utilized. Shutdown and delete the databases used (Amazon Redshift, Amazon RDS, Snowflake). In addition, delete the data in Amazon S3 and make sure to stop any SageMaker Studio notebook instances to not incur any further charges. If you used SageMaker Jumpstart to deploy large language model as SageMaker Real-time Endpoint, delete endpoint either through SageMaker console, or through Studio. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1822e8-f8de-48e3-9aa1-fcfed66edc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Base Python 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-base-python-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
